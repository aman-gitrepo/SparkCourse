{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1a2ff08-7067-455f-9187-4f3644490f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType , LongType\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2597513c-6e1d-47b3-accc-0efcb6031124",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('SparkSQL').getOrCreate()\n",
    "\n",
    "def mapper(line):\n",
    "    fields = line.split(',')\n",
    "    return Row(ID = int(fields[0]),\\\n",
    "               name = str(fields[1].encode('utf-8')),\\\n",
    "               age = int(fields[2]),\\\n",
    "               numFriends=int(fields[3]))\n",
    "\n",
    "lines = spark.sparkContext.textFile('Data/fakefriends.csv')\n",
    "\n",
    "people = lines.map(mapper)\n",
    "\n",
    "\n",
    "schemaPeople = spark.createDataFrame(people).cache()\n",
    "schemaPeople.createOrReplaceTempView('people')\n",
    "\n",
    "teenagers = spark.sql('select * from people where age >=13')\n",
    "\n",
    "for teen in teenagers.collect():\n",
    "    print (teen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1e8258-2ab2-48f1-b2bd-4e14802af198",
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaPeople.groupBy('age').count().orderBy('Age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a20348b-193a-46a6-8667-be232dd42688",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MinTemperatures\").getOrCreate()\n",
    "\n",
    "schema = StructType([ \\\n",
    "                     StructField(\"stationID\", StringType(), True), \\\n",
    "                     StructField(\"date\", IntegerType(), True), \\\n",
    "                     StructField(\"measure_type\", StringType(), True), \\\n",
    "                     StructField(\"temperature\", FloatType(), True)])\n",
    "\n",
    "# // Read the file as dataframe\n",
    "df = spark.read.schema(schema).csv(\"Data/1800.csv\")\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2689f283-a4eb-4dc6-98e6-d987c8cd5c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out all but TMIN entries\n",
    "minTemps = df.filter(df.measure_type == \"TMIN\")\n",
    "# Select only stationID and temperature\n",
    "stationTemps = minTemps.select(\"stationID\", \"temperature\")\n",
    "# Aggregate to find minimum temperature for every station\n",
    "minTempsByStation = stationTemps.groupBy(\"stationID\").min(\"temperature\")\n",
    "minTempsByStation.show()\n",
    "\n",
    "# Convert temperature to fahrenheit and sort the dataset\n",
    "minTempsByStationF = minTempsByStation.withColumn(\"temperature\",\n",
    "                                                  func.round(func.col(\"min(temperature)\") * 0.1 * (9.0 / 5.0) + 32.0, 2))\\\n",
    "                                                  .select(\"stationID\", \"temperature\").sort(\"temperature\")                                              \n",
    "# Collect, format, and print the results\n",
    "results = minTempsByStationF.collect()\n",
    "for result in results:\n",
    "    print(result[0] + \"\\t{:.2f}F\".format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68531505-f6a9-4b18-91ab-455364320530",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_movie = StructType([StructField('userID',IntegerType(),True),\\\n",
    "                          StructField('movieID',IntegerType(),True),\\\n",
    "                          StructField('rating',IntegerType(),True),\\\n",
    "                          StructField('timestamp',IntegerType(),True)])\n",
    "df_movies = spark.read.option('delimiter','\\t').schema(schema_movie).csv(\"ml-100k/u.data\")\n",
    "result = df_movies.groupBy('movieID').count().orderBy(func.desc('count'))\n",
    "# result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1567f0b-9a36-4b34-997e-27cc3e05d352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----------------------------+\n",
      "|movieID|count|movieTitle                   |\n",
      "+-------+-----+-----------------------------+\n",
      "|50     |583  |Star Wars (1977)             |\n",
      "|258    |509  |Contact (1997)               |\n",
      "|100    |508  |Fargo (1996)                 |\n",
      "|181    |507  |Return of the Jedi (1983)    |\n",
      "|294    |485  |Liar Liar (1997)             |\n",
      "|286    |481  |English Patient, The (1996)  |\n",
      "|288    |478  |Scream (1996)                |\n",
      "|1      |452  |Toy Story (1995)             |\n",
      "|300    |431  |Air Force One (1997)         |\n",
      "|121    |429  |Independence Day (ID4) (1996)|\n",
      "+-------+-----+-----------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "\n",
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    # CHANGE THIS TO THE PATH TO YOUR u.ITEM FILE:\n",
    "    with codecs.open(\"ml-100k/u.ITEM\", \"r\", encoding='ISO-8859-1', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            fields = line.split('|')\n",
    "            movieNames[int(fields[0])] = fields[1]\n",
    "    return movieNames\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PopularMovies\").getOrCreate()\n",
    "\n",
    "nameDict = spark.sparkContext.broadcast(loadMovieNames())\n",
    "\n",
    "# Create schema when reading u.data\n",
    "schema = StructType([ \\\n",
    "                     StructField(\"userID\", IntegerType(), True), \\\n",
    "                     StructField(\"movieID\", IntegerType(), True), \\\n",
    "                     StructField(\"rating\", IntegerType(), True), \\\n",
    "                     StructField(\"timestamp\", LongType(), True)])\n",
    "\n",
    "# Load up movie data as dataframe\n",
    "moviesDF = spark.read.option(\"sep\", \"\\t\").schema(schema).csv(\"ml-100k/u.data\")\n",
    "\n",
    "movieCounts = moviesDF.groupBy(\"movieID\").count()\n",
    "\n",
    "# Create a user-defined function to look up movie names from our broadcasted dictionary\n",
    "# def lookupName(movieID):\n",
    "#     return nameDict.value[movieID]\n",
    "\n",
    "# lookupNameUDF = func.udf(lookupName)\n",
    "lookupNameUDF = func.udf(lambda x: nameDict.value[x])\n",
    "\n",
    "# Add a movieTitle column using our new udf\n",
    "moviesWithNames = movieCounts.withColumn(\"movieTitle\", lookupNameUDF(col(\"movieID\")))\n",
    "\n",
    "# Sort the results\n",
    "sortedMoviesWithNames = moviesWithNames.orderBy(func.desc(\"count\"))\n",
    "\n",
    "# Grab the top 10\n",
    "sortedMoviesWithNames.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d281d486-ff7f-4d42-9a74-db7da56cc4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadMovieNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c4a3aa0-e680-46fa-87eb-2bdea206ce21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPTAIN AMERICA is the most popular superhero with 1933 co-appearances.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"MostPopularSuperhero\").getOrCreate()\n",
    "\n",
    "schema = StructType([ \\\n",
    "                     StructField(\"id\", IntegerType(), True), \\\n",
    "                     StructField(\"name\", StringType(), True)])\n",
    "\n",
    "names = spark.read.schema(schema).option(\"sep\", \" \").csv(\"Data/Marvel+Names\")\n",
    "\n",
    "lines = spark.read.text(\"Data/Marvel+Graph\")\n",
    "# Small tweak vs. what's shown in the video: we trim each line of whitespace as that could\n",
    "# throw off the counts.\n",
    "connections = lines.withColumn(\"id\", func.split(func.trim(func.col(\"value\")), \" \")[0]) \\\n",
    "    .withColumn(\"connections\", func.size(func.split(func.trim(func.col(\"value\")), \" \")) - 1) \\\n",
    "    .groupBy(\"id\").agg(func.sum(\"connections\").alias(\"connections\"))\n",
    "mostPopular = connections.sort(func.col(\"connections\").desc()).first()\n",
    "mostPopularName = names.filter(func.col(\"id\") == mostPopular[0]).select(\"name\").first()\n",
    "print(mostPopularName[0] + \" is the most popular superhero with \" + str(mostPopular[1]) + \" co-appearances.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "357b9ca3-2ec8-4be2-8d0f-8072ffb280b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following characters have only 0 connection(s):\n",
      "+--------------------+\n",
      "|                name|\n",
      "+--------------------+\n",
      "|        BERSERKER II|\n",
      "|              BLARE/|\n",
      "|MARVEL BOY II/MARTIN|\n",
      "|MARVEL BOY/MARTIN BU|\n",
      "|      GIURESCU, RADU|\n",
      "|       CLUMSY FOULUP|\n",
      "|              FENRIS|\n",
      "|              RANDAK|\n",
      "|           SHARKSKIN|\n",
      "|     CALLAHAN, DANNY|\n",
      "|         DEATHCHARGE|\n",
      "|                RUNE|\n",
      "|         SEA LEOPARD|\n",
      "|         RED WOLF II|\n",
      "|              ZANTOR|\n",
      "|JOHNSON, LYNDON BAIN|\n",
      "|          LUNATIK II|\n",
      "|                KULL|\n",
      "|GERVASE, LADY ALYSSA|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"MostObscureSuperheroes\").getOrCreate()\n",
    "\n",
    "schema = StructType([ \\\n",
    "                     StructField(\"id\", IntegerType(), True), \\\n",
    "                     StructField(\"name\", StringType(), True)])\n",
    "\n",
    "names = spark.read.schema(schema).option(\"sep\", \" \").csv(\"Data/Marvel+Names\")\n",
    "\n",
    "lines = spark.read.text(\"Data/Marvel+Graph\")\n",
    "\n",
    "# Small tweak vs. what's shown in the video: we trim whitespace from each line as this\n",
    "# could throw the counts off by one.\n",
    "connections = lines.withColumn(\"id\", func.split(func.trim(func.col(\"value\")), \" \")[0]) \\\n",
    "    .withColumn(\"connections\", func.size(func.split(func.trim(func.col(\"value\")), \" \")) - 1) \\\n",
    "    .groupBy(\"id\").agg(func.sum(\"connections\").alias(\"connections\"))\n",
    "    \n",
    "minConnectionCount = connections.agg(func.min(\"connections\")).first()[0]\n",
    "\n",
    "minConnections = connections.filter(func.col(\"connections\") == minConnectionCount)\n",
    "\n",
    "minConnectionsWithNames = minConnections.join(names, \"id\")\n",
    "\n",
    "print(\"The following characters have only \" + str(minConnectionCount) + \" connection(s):\")\n",
    "\n",
    "minConnectionsWithNames.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5c35d19-5734-437c-9ac7-f6d7f3c9719f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BFS iteration# 1\n",
      "Processing 8330 values.\n",
      "Running BFS iteration# 2\n",
      "Processing 220615 values.\n",
      "Hit the target character! From 1 different direction(s).\n"
     ]
    }
   ],
   "source": [
    "#Boilerplate stuff:\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# conf = SparkConf().setMaster(\"local\").setAppName(\"DegreesOfSeparation\")\n",
    "# sc = SparkContext(conf = conf)\n",
    "\n",
    "# The characters we wish to find the degree of separation between:\n",
    "startCharacterID = 5306 #SpiderMan\n",
    "targetCharacterID = 14  #ADAM 3,031 (who?)\n",
    "\n",
    "# Our accumulator, used to signal when we find the target character during\n",
    "# our BFS traversal.\n",
    "hitCounter = sc.accumulator(0)\n",
    "\n",
    "def convertToBFS(line):\n",
    "    fields = line.split()\n",
    "    heroID = int(fields[0])\n",
    "    connections = []\n",
    "    for connection in fields[1:]:\n",
    "        connections.append(int(connection))\n",
    "\n",
    "    color = 'WHITE'\n",
    "    distance = 9999\n",
    "\n",
    "    if (heroID == startCharacterID):\n",
    "        color = 'GRAY'\n",
    "        distance = 0\n",
    "\n",
    "    return (heroID, (connections, distance, color))\n",
    "\n",
    "\n",
    "def createStartingRdd():\n",
    "    inputFile = sc.textFile(\"Data/Marvel+Graph\")\n",
    "    return inputFile.map(convertToBFS)\n",
    "\n",
    "def bfsMap(node):\n",
    "    characterID = node[0]\n",
    "    data = node[1]\n",
    "    connections = data[0]\n",
    "    distance = data[1]\n",
    "    color = data[2]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    #If this node needs to be expanded...\n",
    "    if (color == 'GRAY'):\n",
    "        for connection in connections:\n",
    "            newCharacterID = connection\n",
    "            newDistance = distance + 1\n",
    "            newColor = 'GRAY'\n",
    "            if (targetCharacterID == connection):\n",
    "                hitCounter.add(1)\n",
    "\n",
    "            newEntry = (newCharacterID, ([], newDistance, newColor))\n",
    "            results.append(newEntry)\n",
    "\n",
    "        #We've processed this node, so color it black\n",
    "        color = 'BLACK'\n",
    "\n",
    "    #Emit the input node so we don't lose it.\n",
    "    results.append( (characterID, (connections, distance, color)) )\n",
    "    return results\n",
    "\n",
    "def bfsReduce(data1, data2):\n",
    "    edges1 = data1[0]\n",
    "    edges2 = data2[0]\n",
    "    distance1 = data1[1]\n",
    "    distance2 = data2[1]\n",
    "    color1 = data1[2]\n",
    "    color2 = data2[2]\n",
    "\n",
    "    distance = 9999\n",
    "    color = color1\n",
    "    edges = []\n",
    "\n",
    "    # See if one is the original node with its connections.\n",
    "    # If so preserve them.\n",
    "    if (len(edges1) > 0):\n",
    "        edges.extend(edges1)\n",
    "    if (len(edges2) > 0):\n",
    "        edges.extend(edges2)\n",
    "\n",
    "    # Preserve minimum distance\n",
    "    if (distance1 < distance):\n",
    "        distance = distance1\n",
    "\n",
    "    if (distance2 < distance):\n",
    "        distance = distance2\n",
    "\n",
    "    # Preserve darkest color\n",
    "    if (color1 == 'WHITE' and (color2 == 'GRAY' or color2 == 'BLACK')):\n",
    "        color = color2\n",
    "\n",
    "    if (color1 == 'GRAY' and color2 == 'BLACK'):\n",
    "        color = color2\n",
    "\n",
    "    if (color2 == 'WHITE' and (color1 == 'GRAY' or color1 == 'BLACK')):\n",
    "        color = color1\n",
    "\n",
    "    if (color2 == 'GRAY' and color1 == 'BLACK'):\n",
    "        color = color1\n",
    "\n",
    "    return (edges, distance, color)\n",
    "\n",
    "\n",
    "#Main program here:\n",
    "iterationRdd = createStartingRdd()\n",
    "\n",
    "for iteration in range(0, 10):\n",
    "    print(\"Running BFS iteration# \" + str(iteration+1))\n",
    "\n",
    "    # Create new vertices as needed to darken or reduce distances in the\n",
    "    # reduce stage. If we encounter the node we're looking for as a GRAY\n",
    "    # node, increment our accumulator to signal that we're done.\n",
    "    mapped = iterationRdd.flatMap(bfsMap)\n",
    "\n",
    "    # Note that mapped.count() action here forces the RDD to be evaluated, and\n",
    "    # that's the only reason our accumulator is actually updated.\n",
    "    print(\"Processing \" + str(mapped.count()) + \" values.\")\n",
    "\n",
    "    if (hitCounter.value > 0):\n",
    "        print(\"Hit the target character! From \" + str(hitCounter.value) \\\n",
    "            + \" different direction(s).\")\n",
    "        break\n",
    "\n",
    "    # Reducer combines data for each character ID, preserving the darkest\n",
    "    # color and shortest path.\n",
    "    iterationRdd = mapped.reduceByKey(bfsReduce)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6b78e2-2584-4dcd-a178-ddfeffa18071",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

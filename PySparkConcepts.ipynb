{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f2689d6-9325-47f5-ab25-adaa7f818133",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    " the unified entry point of Spark.\r\n",
    " the unified entry point of Spark.\r\n",
    " the unified entry point of Spark.\r\n",
    " the unified entry point of Spark.\r\n",
    " the unified entry point of Spark.\r\n",
    "\r\n",
    " the unified entry point of Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3c2142-ac6f-463c-9bdf-f6500464cd40",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Imports in PySpark\n",
    "\n",
    "* **pyspark.sql.SparkSession**            – SparkSession is the main entry point for DataFrame and SQL functionality.\n",
    "* **pyspark.sql.DataFrame**               – DataFrame is a distributed collection of data organized into named columns.\n",
    "* **pyspark.sql.Column**                  – A column expression in a DataFrame.\n",
    "* **pyspark.sql.Row**                     – A row of data in a DataFrame.\n",
    "* **pyspark.sql.GroupedData**             – An object type that is returned by DataFrame.groupBy().\n",
    "* **pyspark.sql.DataFrameNaFunctions**    – Methods for handling missing data (null values).\n",
    "* **pyspark.sql.DataFrameStatFunctions**  – Methods for statistics functionality.\n",
    "* **pyspark.sql.functions**               – List of standard built-in functions.\n",
    "* **pyspark.sql.types**                   – Available SQL data types in PySpark.\n",
    "* **pyspark.sql.Window**                  – Would be used to work with window functions.\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "* \tStringType\tShortType\n",
    "* \tArrayType\tIntegerType\n",
    "* \tMapType\tLongType\n",
    "* \tStructType\tFloatType\n",
    "* \tDateType\tDoubleType\n",
    "* \tTimestampType\tDecimalType\n",
    "* \tBooleanType\tByteType\n",
    "* \tCalendarIntervalType\tHiveStringType\n",
    "* \tBinaryType\tObjectType\n",
    "* \tNumericType\tNullType\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "* \tcol\n",
    "* \trand\n",
    "* \twhen\n",
    "* \t\n",
    "\n",
    "\n",
    "## Notes\n",
    "\n",
    "*sparkContext* is a Scala implementation entry point and JavaSparkContext is a java wrapper of sparkContext.\n",
    "\n",
    "*SQLContext* is entry point of SparkSQL which can be received from sparkContext.\n",
    "> Prior to 2.x.x, RDD ,DataFrame and Data-set were three different data abstractions.Since spark2 all are unified under sparksession"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
